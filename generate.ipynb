{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # Puts each data field into a tensor with outer dimension batch size\n",
    "    assert isinstance(batch[0], tuple)\n",
    "    trans = torchvision.transforms.ToTensor()\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    num_total_points = 784\n",
    "    num_context = 10 # half of total points\n",
    "    \n",
    "    context_x, context_y, target_x, target_y = list(), list(), list(), list()\n",
    "    \n",
    "    for d, _ in batch:\n",
    "        d = trans(d)\n",
    "        total_idx = range(784)\n",
    "        total_idx = list(map(lambda x: (x//28, x%28), total_idx))\n",
    "        c_idx = np.random.choice(range(784), num_total_points, replace=False)\n",
    "        c_idx = list(map(lambda x: (x//28, x%28), c_idx))\n",
    "        c_idx = c_idx[:num_context]\n",
    "        c_x, c_y, total_x, total_y = list(), list(), list(), list()\n",
    "        for idx in c_idx:\n",
    "            c_y.append(d[:, idx[0], idx[1]])\n",
    "            c_x.append((idx[0] / 27., idx[1] / 27.))\n",
    "        for idx in total_idx:\n",
    "            total_y.append(d[:, idx[0], idx[1]])\n",
    "            total_x.append((idx[0] / 27., idx[1] / 27.))\n",
    "        c_x, c_y, total_x, total_y = list(map(lambda x: t.FloatTensor(x), (c_x, c_y, total_x, total_y)))\n",
    "        context_x.append(c_x)\n",
    "        context_y.append(c_y)\n",
    "        target_x.append(total_x)\n",
    "        target_y.append(total_y)\n",
    "        \n",
    "        \n",
    "    context_x = t.stack(context_x, dim=0)\n",
    "    context_y = t.stack(context_y, dim=0).unsqueeze(-1)\n",
    "    target_x = t.stack(target_x, dim=0)\n",
    "    target_y = t.stack(target_y, dim=0).unsqueeze(-1)\n",
    "    \n",
    "    return context_x, context_y, target_x, target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import LatentModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentModel(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentModel(\n",
       "  (latent_encoder): LatentEncoder(\n",
       "    (input_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=3, out_features=128, bias=True)\n",
       "    )\n",
       "    (self_attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (penultimate_layer): Linear(\n",
       "      (linear_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (mu): Linear(\n",
       "      (linear_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (log_sigma): Linear(\n",
       "      (linear_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (deterministic_encoder): DeterministicEncoder(\n",
       "    (self_attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (cross_attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Attention(\n",
       "        (key): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (value): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (multihead): MultiheadAttention(\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (residual_dropout): Dropout(p=0.1)\n",
       "        (final_linear): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (input_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=3, out_features=128, bias=True)\n",
       "    )\n",
       "    (context_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2, out_features=128, bias=True)\n",
       "    )\n",
       "    (target_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (target_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2, out_features=128, bias=True)\n",
       "    )\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(\n",
       "        (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (1): Linear(\n",
       "        (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (2): Linear(\n",
       "        (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (final_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=384, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (BCELoss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = t.load('./checkpoint/checkpoint_50.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict=state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# fig1 = plt.figure()\n",
    "model.train(False)\n",
    "for i, d in enumerate(dloader):\n",
    "    context_x, context_y, target_x, target_y = d\n",
    "    pred_y, _, _ = model(context_x, context_y, target_x, None) # Test\n",
    "    \n",
    "    fig.add_subplot(1,10,i+1)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(t.sigmoid(target_y).squeeze(0).view(-1,28).detach().numpy())\n",
    "#     plt.imshow(t.sigmoid(target_y).squeeze(0).view(-1,28))\n",
    "#     plt.imshow(pred_y.squeeze(0).view(28,28).detach().numpy())\n",
    "    if i == 9:\n",
    "        \n",
    "        break\n",
    "\n",
    "plt.savefig('./results/original.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL.soobinsuh",
   "language": "python",
   "name": "soobinsuh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
